linux设备3种类型
1字符设备：必须以串行顺序依次进行访问的设备
2块设备：可以按任意顺序进行访问，以块为单位进行操作
12都使用文件系统接口编程open，close，read，write
3网络设备：面向数据包的接收和发送，不适用文件系统接口编程，使用socket

块设备的访问有两种方式
1使用原始设备访问如/dev/sdb
2使用文件系统虚拟出的文件名称访问

冯诺依曼结构
存储器的程序和数据在一起，共用总线（地址和数据）
哈佛结构
存储器的程序和数据分开，独立使用总线（地址和数据）
改进的模型程序和数据存储器分开，但是复用总线（地址和数据）

通用处理器GPP
数字信号处理器DSP
硬件乘法器
卷积，数字滤波，FFT，矩阵运算
浮点DSP硬件浮点运算
专用处理器ASP及ASIC

CFI
SRAM NOR
片上执行
NAND只可块操作，专用接口

对于NOR，NAND等提供了MTD子系统，其上运行的YFSS2，JFFS3，UBIFS等具备擦出和负载均衡能力的文件系统

外设接口
RS232 RS485
I2C SPI CAN
USB SDIO LAN
EMMC I3C MIPI

linux2.6内核
调度器CFS算法EDF算法
内核抢占，线程模型NPTL
虚拟内存 反向映射
文件系统 btrfs
总线、设备、驱动模型

驱动drivers与arch独立，kernel fs ipc net等与硬件剥离
进程调度 内存管理 VFS net IPC
进程调度 CFS算法EDF算法
内存管理buddy算法slab flusher kswapd
VFS 为各个不同文件系统封装为统一接口，还有利用net的NFS，利用mm的ramdisk
NET
IPC 信号量，共享内存，消息队列，管道，UNIX域套接字

早期bootloader以ATAG格式形式封装启动信息，把其地址放入r2寄存器，linux支持设备树后，bootloader把dtb地址放入r2，可以通过配置把dtb和zImage绑定，r2不需要放dtb地址了
bootrom--bootloader--linux--init运行并生成进程树
arm-linux-guneabihf-gcc使用硬件浮点FPU处理浮点数比arm-linux-guneabi-gcc编译的程序处理浮点数运算的性能好得多

驱动模块
insmod modprobe加载模块，rmmod卸载模块
驱动程序中使用request_module(module_name)加载模块
声明时加_init的函数和声明时加_initdata的变量将在初始化完成后释放内存空间
模块卸载函数和变量中加_exit _exitdata表示如果模块直接编译进内核时，这些变量和函数将被忽略，因为不会使用卸载函数
module_param()用于从insmod modprobe参数中获取参数，当编译进内核时，可以从bootargs获取
/proc/kallsyms存放内核符号表及其地址，导出的符号可以被其他模块使用，声明一下即可EXPORT_SYMBOL,EXPORT_SYMBOL_GPL到处的符号表不能被非GPL的模块引用
MODULE_LISENCE,为了使用GPL的符号表，非GPL的模块经常把GPL的符号表封装一层使用EXPORT_SYMBOL导出
作者MODULE_AUTHOR，描述MODULE_DESCRIPTION，版本MODULE_VERSION，设备表MODULE_DEVICE_TABLE，别名MODULE_ALIAS
SMP和PREEMPT
模块计数用于管理模块是否被使用，被使用时无法卸载模块
linux文件操作creat umask open read write lseek close
c库文件操作fopen fgetc fputc fgets fputs fprintf fscanf fread fwrite fgetpos fsetpos fseek fclose
fread如果返回值不等与字段数，可能时出错或者文件末尾，需要调用feof或ferror查看
VFS与应用之间的接口是以上系统调用，与内核接口是file_operations
目录/proc目录存放进程及内核信息，这个目录并不是真正的文件，他存在于内存中，/sys目录存放sysfs，虚拟的，linux设备驱动模型中的总线驱动和设备都有节点，总线设备类
文件file结构体和inode结构体
linux2.6以后udev取代devfs
注册和注销字符设备register_chrdev,unregister_chrdev
linux设计的一个基本观点是机制和策略分离，机制是做事情的步骤方法，是固定的，策略是每个步骤的具体实现，是不固定的，因此linux内核中不应实现策略
udev完全在用户态工作，发送的事件uevent
总线bus_type,驱动device_driver,设备device结构体
设备和驱动都依附于总线，因此设备和驱动结构体中都有总线的实例，设备和驱动分离，通过总线match函数检测到匹配的设备和驱动时，才执行驱动的probe函数，
总线有platform，pci，i2c，usb等，总线设备驱动最终落实在sysfs，attribute落实为sysfs中的一个文件
udev动态建立删除设备文件，内核检测到设备后，会使用netlink发送uevent，udev通过内核发送的内容进行匹配，例如插入u盘，udev规则文件定义新设备处理规则
udevadm info -a -p /sys/device/platform/serial8250/tty/ttyS0
udev的轻量级版本mdev

字符设备驱动：cdev结构体
cdev_init建立设备和file_operation产生联系
cdev_alloc开辟cdev空间
cdev_add,cdev_dev设备添加和删除
register_chrdev_region申请设备号
alloc_chrdev_region申请设备号，设备号起始地址未知
unregister_chrdev_region释放设备号

file_operation结构体
llseek read write（read write返回0表示EOF）
unlocked_ioctl和用户空间fcntl和ioctl对应
mmap将设备内存映射到进程的虚拟地址空间，帧缓冲驱动中使用其直接修改显示内容
open release
poll和用户空间的poll和select对应
aio_read aio_write异步读写，对应用户空间SYS_io_setup,SYS_io_submit,SYS_io_getevents,SYS_io_destroy
用户空间不能访问内核空间内存，使用copy_from_user,copy_to_user转换
内核空间可以访问用户空间内存，使用access_ok检查是否是用户空间内存，如果不检查，入侵者可以传入一个内核空间地址，让驱动给其填充数据

ioctl的参数cmd
设备类型（魔数），序列号，方向，长度，使用_IO() _IOR() _IOW() _IOWR()四个宏定义生成cmd，内核中有一些专用的控制命令无法被驱动使用
设备驱动中常把私有数据private_data指向设备结构体，再用read write ioctl lseek访问private_data
private_data的设置可以在open函数中
使用private_data的好处是可以轻易的修改为支持多个同类设备的驱动
container_of是通过成员指针获取结构体指针

加载模块，创建节点mknod
字符驱动 初始化，添加删除cdev结构体，申请释放设备号，填充file_operations函数

并发和竞态
并发：多个执行单元同时并行运行
竞态：同时并行运行的执行单元访问共享资源（硬件，软件上的全局变量和静态变量）
进程和进程间（多处理器和单处理器时间片），进程和中断间，中断和中断间（中断嵌套，2.6.35之后取消了中断嵌套，之前可以用IRQF_DISABLE避免嵌套）
解决竞态的办法是互斥访问共享资源，访问共享资源的代码区域叫做临界区，临界区需要互斥机制保护（中断屏蔽，原子操作，自旋锁，信号量，互斥体）

编译乱序和执行乱序
编译乱序：打开编译器优化后，编译器会对访存指令乱序提高cache命中率，可以使用barrier编译屏障防止编译乱序，barrier可以保证其之前和之后的程序不乱序
volatile也具有防止编译乱序的弱作用，比如线程两次访问某变量，如果不加volatile，可能第二次会直接优化掉使用第一次的结果，volatile不能保护临界资源
linux内核中不太喜欢volatile
执行乱序：当一条指令访存时间较长时，如果后面的指令不依赖于前面的执行结构，可能会同时执行后面指令，ARM926EJ-S没有执行乱序功能，新核有
可以使用内存屏障解决执行乱序造成的问题，当访问外设寄存器时，这些外设寄存器对CPU逻辑上无法构成依赖，可能会出现内存乱序，当这些寄存器的访问需要顺序
执行时，需要使用内存屏障DMB,DSB,ISB，读写寄存器readl,readl_relaxed, writel,writel_relaxed,前边的有内存屏障，后边的没有
当开启dma时，先使用writel_relaxed写入地址大小等，再使用writel是能dma，防止乱序造成先使能dma后配置

中断屏蔽：单片机等单核cpu中常在进入临界区前屏蔽中断解决竞态问题，linux内核驱动为了保证可以执行一般不这样做
local_irq_disable,local_irq_enable,中断屏蔽实际是解决进程和中断间的竞态，而linux的进程调度也是以来中断实现，因此也解决了进程和进程间的竞态，
但是只能屏蔽本cpu的中断，在不能解决SMP中的竞态，应与自旋锁联合使用

原子操作：在arm中使用LDREX,STREX指令实现，实现单cpu和多cpu的原子操作
linux中有整形原子操作atomic_set,atomic_read,atomic_add,atomic_sub,atomic_inc,atomic_dec,
位原子操作set_bit,clear_bit,change_bit,test_bit

自旋锁：获取自旋锁是一个原子操作，当锁被获取后，如果其他执行单元需要获取锁，则一直判断锁是否被释放，直到其被释放，自旋锁持有期间内核抢占被禁止，
SMP中只有当前核的抢占被禁止。
spinlock_t lock;spin_lock_init(lock);spin_lock(lock);spin_trylock(lock)实际不自旋;spin_unlock(lock);
自旋锁虽然不会被进程调度影响，但是会被中断和底半部影响，因此需要配合关中断local_irq_disable，关底半部local_bh_disable,关中断并保存状态local_irq_save使用
形成了spin_lock_irq,spin_unlock_irq;spin_lock_irqsave,spin_unlock_irqrestore;spin_lock_bh,spin_unlock_bh;
进程中使用spin_lock_irqsave,中断中使用spin_lock,这样进程cpu核的中断核抢占被禁止，而其他核获取spin_lock会失败
使用自旋锁的注意事项1自旋锁是死等，适用于临界区较短，不适用于临界区大或者有共享设备时2死锁，当一个进程获取自旋锁后再次获取3获取自旋锁后不能调用
copy_to_user,copy_from_user,kmalloc,msleep等可以导致阻塞的函数4中断里使用spin_lock防止多处理器时，中断在其他核运行
使用自旋锁使设备只能被一个进程打开，在open核release函数中使用自旋锁判断使用计数
读写自旋锁：rwlock_t lock;rwlock_init(lock),read_lock(),read_lock_irqsave,read_lock_irq,read_lock_bh,read_unlock(),写锁一样，与自旋锁使用方法相同
多读单写，读写不能同时
顺序锁：write_seqlock,write_tryseqlock,write_seqlock_irqsave,write_seqlock_irq,write_seqlock_bh,write_sequnlock
read_seqbegin,read_seqbegin_irqsave,配合read_seqretry,read_seqretry_irqrestore使用，如果读过程中被写，会执行重读
多读单写，读写同时进行，读时如果写，则循环重读
RCU锁：读复制更新锁，读不需要加锁，写是先读，然后拷贝到一个副本，修改副本后，等所有处理器不在引用数据时，更新回原数据
rcu_read_lock,rcu_read_unlock,synchronize_rcu(),call_rcu,rcu_assign_pointer,rcu_dereference,rcu_access_pointer以及操作链表的函数
读写同时，读效率高，写效率低，不适用与写入频繁的操作;
信号量：P获取信号量，V释放信号量sem_init(),down()会导致睡眠，不能在中断里使用，睡眠中不能被信号打断，down_interruptible可以被信号打断
down_trylock(),up()释放信号量，信号量在获取不到时会睡眠，而自旋锁获取不到会等待，信号量常用于解决生产者消费者问题，信号量内核中多被mutex取代
互斥体：mutex_init()，mutex_lock()导致睡眠不能打断,mutex_trylock(),mutex_lock_interruptible()，mutex_unlock()
自旋锁和互斥体运用最广泛
1互斥体是进程级的，适用于临界区较大的情况，自旋锁适用于临界区小的情况
2互斥体保护的临界区可以有引起阻塞的代码，但是自旋锁不行，可能导致死锁
3如果是在中断里，只能使用自旋锁，当然也可用mutex_trylock()
完成量：一个执行单元等待另一个执行单元完成某事init_completion(),wait_for_completion(),complete(),complete_all();

阻塞与非阻塞IO
open使用O_NONBLOCK参数时为非阻塞，或者使用fcntl设置，否则为阻塞
使用等待队列实现阻塞IO，init_waitqueue_head(),add_wait_queue(),remove_wait_queue();
wait_event(),wait_event_interruptible(),wait_event_timeout(),wait_event_interruptible_timeout(),interruptible表示可被信号打断，timeout表示可超时退出
wake_up(),唤醒处于可打断和不可打断的进程，wake_up_interruptible()打断可被信号打断的进程与wait_函数对应使用
sleep_on(),interruptible_sleep_on()把进程状态设置为TASK_UNINTERRUPTIBLE和TASK_INTERRUPTIBLE
使用schedule切换进程，一般判断是否可读或可写后，判断是否非阻塞读写，然后设置进程状态，然后切换进程
__set_current_state浅度睡眠标记，并不真正睡眠，schedule后才切换进程，schedule前一定要释放互斥体，否则会死锁。

轮询操作
select和poll
应用层select（maxfd+1，*readfds，*writefds，*exceptfds，*timeout）分辨监控读写和异常处理文件描述符的变化
应用调用select的时候，实际每个驱动的poll函数被调用，进程被挂在每个驱动的等待队列上。
poll和select原理类似，当文件描述符数量庞大时，poll和select性能会降低，epoll性能会优于他们，底层自建红黑树？
epoll_create(size);入参监视fd的数量，返回一个epfd
epoll_ctl(epfd，op，fd，*events);
event有EPOLLIN,EPOLLOUT,EPOLLPRI,EPOLLERR,EPOLLHUP,EPOLLLET边沿触发，只通知一次，默认水平触发，如果不操作fd，事件一直存在。
EPOLLONESHOT只监听一次，如果事件触发后还想监听，需重新加到epoll队列
epoll_wait(epfd,*events,maxevents,timeout);
驱动中的xxx_poll，poll_wait
阻塞IO通常使用等待队列实现。至于应用层使用epoll还是select，驱动层的程序时相同的，可能内核中的处理会有差别

异步通知与异步IO
异步通知，相当于中断，应用不用轮询或阻塞，驱动准备好后通知应用。
应用层signal(SIGINT,sig_handler),sigaction()
使用fcntl设置进程拥有信号，设置异步通知
驱动层fasync(),kill_fasync()触发信号。
异步IO，aio_read()异步读,aio_write()异步写,aio_error()异步操作是否完成,aio_return()异步操作返回值
aio_suspend()阻塞进程，等待异步操作完成，aio_cancel()取消请求
lio_listio()等待多个异步操作完成
内核层，块驱动，应用程序减少同步负载，并发控制，负载均衡，提高socket吞吐性能
用户空间使用libaio完成异步操作io_setup,io_submit,io_getevents,io_destroy
驱动层file_ops,aio_read,aio_write,aio_fsync,对于块设备和网络设备，一般linux核心层已经实现了异步操作
用户空间设置文件拥有者，fasync标志及捕获信号，内核空间相应对文件的拥有者fasync标志的设置，并在资源可获得时释放信号。

中断
内部中断，外部中断；屏蔽中断，非屏蔽中断；向量中断，非向量中断
linux中断的顶半部和底半部：设计思路是紧急且不可打断的短小操作与耗时而不紧急的操作分离开执行。
顶半部：处理紧急但是不耗时的内容，登记中断，将底半部耗时内容挂载到设备的执行队列
底半部：处理耗时的操作，几乎中断要处理的所有内容，可以被中断。tasklet，工作队列，软中断，线程化irq
申请和释放中断，request_irq申请中断指定顶半部回调函数,free_irq
屏蔽一个中断disable_irq等待已经进入的中断返回，中断里使用会导致死锁，中断里使用disable_irq_nosync()
宏local_irq_save,local_irq_disable,屏蔽本cpu的所有中断。local_irq_restore,local_irq_enable恢复
tasklet就是在顶半部末尾调度tasklet，它会在适当的时候执行。
工作队列执行方式和tasklet相同，但其上下文是内核线程，可以调度和睡眠；工作队列实质上是线程池，和libuv的工作队列是相同的
软中断一般不使用，tasklet基于软中断，不允许阻塞和睡眠
中断，软中断和信号
线程irq，使用request_thread_irq
中断共享，申请中断时使用IRQF_SHARED，条件是中断未申请，或已申请的中断都使用IRQF_SHARED，中断顶半部判断是否为本设备的中断
内核定时器
完成周期性的工作，依靠软中断实现，高精度定时器hrtimer，可以精确到微秒级。
delayed_work使用工作队列和定时器实现。延时执行某些任务或周期执行任务
内核延时 忙等待：ndelay，udelay，mdelay
内核启动时校准calibrating(lpj=11111),是校准内核延时循环校准，bootargs中传入lpj=1111，可省略此过程，节省启动时间百毫秒
睡眠等待：毫秒以上的延时推荐使用msleep，msleep_interuptible,ssleep
长延时time_before(),time_after()
sleep_on_timeout,把进程挂到等待队列，在等待队列上睡眠

内存与IO访问
MMU，虚拟地址物理地址转换，内存访问权限保护，TLB，TTW，cache等
linux内存映射，DMA，常规，高端内存这三个区域都采用buddy算法进行管理，/proc/buddyinfo；
对于DMA和常规内存，使用virt_to_phys,phys_to_virt转换物理和虚拟内存，高端内存不适用
buddy算法：内存页以2的n次方管理，拆分合并，防止出现内存碎片。但是有时会出现浪费，拆分合并有大量链表和位图操作，开销大

内存存取
用户空间使用malloc和free，C库的malloc一般调用brk和mmap从内核申请内存。mallopt可以是free释放的内存仍属于进程，并不还给内核，之后会自行分配释放
linux是按需调页，malloc时并不真正分配页，read时全0，只有当write出现缺页错误时，才会真正分配。
linux内核使用kmalloc和_get_free_pages分配物理上连续的内存vmalloc分配的内存在物理上不一定连续，虚拟内存连续
kmalloc使用GFP_KERNEL标志申请内存时，如果申请不到，会阻塞等待，因此不能在中断和自旋锁中调用
在tasklet和内核定时器等不能阻塞的进程中，使用GFP_ATOMIC申请内存，申请不到页会直接返回。
GFP_DMA,GFP_USER等，对应使用kfree释放；_get_free_pages和kmalloc参数相似，free_page和free_pages释放
slab算法：slab建立在buddy算法之上，拿到内存页后做二次管理，频繁申请和释放的对象放入slab缓存，释放时并不还给buddy，而是留待下次申请继续使用。
kmem_cache_create创建slab缓存，kmem_cache_alloc,kmem_cache_free,kmem_cache_destroy, /proc/slabinfo
内存池：mempool_create，mempool_alloc,mempool_free,mempool_destroy

IO端口和IO内存
linux IO端口 读写8位inb，outb；读写16位io端口inw，outw；读写32位端口inl，outl；读写一串字节insb，outsb；读写一串字insw，outsw，读写一串长字insl，outsl
Linux IO内存：芯片内部IIC，SPI,USB等控制器的寄存器和外部内存总线上的设备。先使用ioremap把物理地址映射到虚拟地址iounmap释放，devm_ioremap不需要释放
读写使用readb,readb_relaxed,readw,readw_relaxed,readl,readl_relaxed,writeb,writeb_relaxed,writew,writew_relaxed,writel,writel_relaxed
带relaxed表示有内存屏障
申请和释放IO端口request_region,release_region,devm_request_region
申请和释放IO内存request_mem_region,release_mem_region,devm_request_mem_region
IO端口访问流程request_region；inb，outb；release_region
IO内存访问流程request_mem_region；ioremap；readb，writeb；iounmap;release_mem_region

设备地址映射到用户空间
mmap将用户空间内存与设备关联，通过访问用户空间内存直接控制设备，framebuffer，munmap
VMA结构体,remap_pfn_range，fault函数在发生缺页时自动调用
ARM核写缓冲器，解放CPU核cache，只用于写主存；pgprot_noncached禁止cache核写缓冲，io内存被映射时需要
IO内存静态映射  map_desc结构体；不推荐

DMA
DMA控制器独立完成内存核外设的数据交换，结束后通过中断通知cpu
DMA与cache一致性，当DMA内存和cache重叠时，DMA操作可能会修改cache内容导致与内存不一致，使能和关闭cache也会导致不一致，需要注意s
scatter/gather方式是与block dma方式
虚拟地址，物理地址：从mmu管理器角度看内存，总线地址：从设备角度看内存；
DMA地址掩码：把地址转化为DMA控制器可使用的dma_set_mask()
一致性DMA缓冲区dma_alloc_coherent,dma_free_coherent
CMA了解
流式DMA映射dma_map_single,dma_unmap_single;SG模式dma_map_sg,dma_unmap_sg
dmaengine标准API
dma_request_channel,dma_request_slave_channel,dma_release_channel

linux设备驱动软件架构
linux设备驱动非常重视可重用和跨平台能力
linux3.x一个镜像适用于多个硬件的目标
linux总线设备驱动模型：驱动只管驱动，设备只管设备，总线负责匹配设备和驱动，驱动通过标准途径获得板级信息
分层设计，不变的部分提炼出来，input核心层
M*N网状耦合，大量重复代码，需要提炼中间层左到M对1，N对1；主机控制器驱动和外设驱动提炼中间层和标准API，使主机控制器驱动和外设驱动可以任意组合而不用修改
总线负责匹配设备和驱动，如PCI，USB,IIC,SPI等设备可以直接使用总线匹配，对于不依赖这些总线的设备，使用虚拟总线，platform总线
与platform_driver地位对等的是IIC_driver,usb_driver,spi_driver,pci_driver都包含device_driver结构体成员
bus_type成员中的match函数用于匹配platform_device和platform_driver
匹配分为四种：1设备树风格匹配2ACPI风格匹配3匹配ID表4设备名和驱动名匹配；匹配后会在/sys/devices/platform/xxx中找到设备驱动
linux2.6x中platform_device通常在BSP文件中，linux3.x后倾向于根据设备树自动展开
resouce结构体也在BSP中定义，指定设备占用内存，中断等资源，platform_get_resources获取资源
与板子有关的数据使用platform_data从BSP传入驱动
platform总线驱动好处1总线匹配设备和驱动2隔离驱动和BSP，驱动一致性更好3一份驱动运行多个设备，只需要配置platform信息即可

linux设备驱动大量使用面向对象设计思想，封装继承多态，核心层重写等，使用分层设计思想，提炼多级核心层
驱动核心层：1对上提供接口file_operation 2对下定义框架xxx_ops 3中间层实现通用代码layer_ops
主机驱动和外设驱动分离 四个模块1主机驱动，2外设驱动 3主机外设接口 4板级信息

input子系统，RTC驱动，framebuffer驱动，tty设备驱动，misc设备驱动，
spi设备驱动：主机端spi_master,外设端spi_driver,主机外设接口spi_transfer，spi_message；spi_message_add_tail,spi_sync,spi_async
spi_driver负责把外设挂接在spi总线上，spi_device信息在BSP中定义spi_board_info，或者dts中
linux中百多个驱动子系统。。。重要的是搞清楚分层，分离的设计原理，以不变应万变

块设备驱动
块设备1以块为单位输入输出2有缓冲区3支持随机访问
与字符设备file_operations类似的block_device_operations
ioctl和compat_ioctl(64位系统的32位进程调用ioctl时使用)通用层实现了大量标准ioctl操作
check_event老的内核在用户空间检查可移动设备，新的内核在内核空间检查。如何实现异步通知，设备插入和移除？
gendisk结构体bio结构体，一个bio对应上层传递给块的io请求，由于其内存页不是连续的，bio可以对应多个内存页，而一个io请求可以包含多个bio，request_queue是请求队列
IO调度器 Noop IO调度器 适合flash  Deadline IO调度器，适合读取较多（数据库） CFQ IO调度器，提供均衡性能，默认调度器
可以通过内核启动参数或者修改sys下文件改变调度器
块设备驱动：制造请求和请求队列以及bio，不像字符设备一样直接处理IO，而是通过制造请求和处理请求及IO调度对IO整合和排队
MMC子系统 大多数SOC继承MMC/SD/SDIO控制器SDHCI

网络设备驱动
分层：1网络协议接口层dev_queue_xmit,netif_rx2网络设备接口层3设备驱动功能层4网络设备媒介层
主要是实现网络设备接口层net_device数据结构的内容，并注册进内核
套接字缓冲区sk_buff结构体
发送：使用netif_wake_queue,netif_stop_queue完成超时重发和发满暂停
接收：中断接收

IIC总线驱动
组成部分：IIC核心是通用平台无关代码，联系总线驱动和设备驱动；IIC总线驱动，实现i2c_adapter,i2c_algorithm平台i2c功能；
IIC设备驱动定义并实现i2c_driver，实现具体设备的驱动，平台无关
数据结构：i2c_adapter,i2c_algorithm,i2c_driver,i2c_client,i2c_msg
i2c-dev.c为应用提供了主设备号为89的通用操作i2c的方法
应用层读写i2c可以使用read write，也可使用ioctl的I2C_RDWR,驱动需要实现，也可使用ioctl设置从机地址，超时，重发次数等

USB主机设备和Gadget驱动
USB总线：树型拓扑，主从式
usb主机控制器驱动，usb核心，usb设备驱动
UDC驱动程序，Gadget Function API，Gadget Function驱动程序
设备，配置，接口，端点 四个层次使用设备描述符usb_device_descriptor,配置描述符usb_config_descriptor,接口描述符usb_interface_descriptor,
端点描述符usb_endpoint_descriptor描述，使用lsusb命令查看设备的描述符
usb_driver结构体是总线结构体，负责匹配设备驱动，调用probe和disconnect插入移除设备
usb请求块URB是描述与usb设备通信的基本载体，类似于网络设备中的sk_buff






